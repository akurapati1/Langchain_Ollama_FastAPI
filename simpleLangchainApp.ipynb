{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACKING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x230a3334500>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='Document loaders | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content=\"Skip to main contentA newer LangChain version is out! Check out the latest version.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersDocument loadersCustom Document LoaderCSVFile DirectoryHTMLJSONMarkdownMicrosoft OfficePDFText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).RetrievalDocument loadersOn this pageDocument loadersinfoHead to Integrations for documentation on built-in document loader integrations with 3rd-party tools.Use document loaders to load data from a source as Document's. A Document is a piece of text\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='implement a \"lazy load\" as well for lazily loading data into memory.Get started\\u200bThe simplest loader reads in a file as text and places it all into one document.from langchain_community.document_loaders import TextLoaderloader = TextLoader(\"./index.md\")loader.load()API Reference:TextLoader[    Document(page_content=\\'---\\\\nsidebar_position: 0\\\\n---\\\\n# Document loaders\\\\n\\\\nUse document loaders to load data from a source as `Document`\\\\\\'s. A `Document` is a piece of text\\\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\\\n\\\\nEvery document loader exposes two methods:\\\\n1. \"Load\": load documents from the configured source\\\\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\\\n\\\\nThey optionally implement:\\\\n\\\\n3. \"Lazy load\": load documents into memory lazily\\\\n\\', metadata={\\'source\\':'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='load documents from the configured source and split them using the passed in text splitter\\\\n\\\\nThey optionally implement:\\\\n\\\\n3. \"Lazy load\": load documents into memory lazily\\\\n\\', metadata={\\'source\\': \\'../docs/docs/modules/data_connection/document_loaders/index.md\\'})]Help us out by providing feedback on this documentation page:PreviousRetrievalNextDocument loadersGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS  \n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x230c3936900>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query =\"The simplest loader reads in a file as text and places it all into one document.\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000023128D8E150>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000230C393DFA0>, root_client=<openai.OpenAI object at 0x0000023128CBD3A0>, root_async_client=<openai.AsyncOpenAI object at 0x0000023128D8F890>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, LangChain includes document loaders that can load documents from various sources and potentially split them using a specified text splitter. Additionally, there is mention of implementing a \"lazy load\" feature for loading data into memory in a more efficient or delayed manner.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Assuming `document_chain` is a LangChain chain object initialized elsewhere\n",
    "response = document_chain.invoke({\n",
    "    \"input\": \"Document loaders provide a load method for\",\n",
    "    \"context\": [\n",
    "        Document(\n",
    "            metadata={\n",
    "                'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/',\n",
    "                'title': 'Document loaders | ü¶úÔ∏èüîó LangChain',\n",
    "                'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.',\n",
    "                'language': 'en'\n",
    "            },\n",
    "            page_content='Document loaders | ü¶úÔ∏èüîó LangChain'\n",
    "        ),\n",
    "        Document(\n",
    "            metadata={\n",
    "                'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/',\n",
    "                'title': 'Document loaders | ü¶úÔ∏èüîó LangChain',\n",
    "                'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.',\n",
    "                'language': 'en'\n",
    "            },\n",
    "            page_content=\"Skip to main contentA newer LangChain version is out! Check out the latest version.ComponentsIntegrationsGuidesAPI ReferenceMore...\"\n",
    "        ),\n",
    "        Document(\n",
    "            metadata={\n",
    "                'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/',\n",
    "                'title': 'Document loaders | ü¶úÔ∏èüîó LangChain',\n",
    "                'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.',\n",
    "                'language': 'en'\n",
    "            },\n",
    "            page_content=\"and associated metadata. For example, there are document loaders for loading a simple .txt file...\"\n",
    "        ),\n",
    "        Document(\n",
    "            metadata={\n",
    "                'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/',\n",
    "                'title': 'Document loaders | ü¶úÔ∏èüîó LangChain',\n",
    "                'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.',\n",
    "                'language': 'en'\n",
    "            },\n",
    "            page_content=\"implement a \\\"lazy load\\\" as well for lazily loading data into memory.Get started\\u200bThe simplest loader reads in a file...\"\n",
    "        ),\n",
    "        Document(\n",
    "            metadata={\n",
    "                'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/',\n",
    "                'title': 'Document loaders | ü¶úÔ∏èüîó LangChain',\n",
    "                'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.',\n",
    "                'language': 'en'\n",
    "            },\n",
    "            page_content=\"load documents from the configured source and split them using the passed in text splitter...\"\n",
    "        )\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000230C3936900>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000023128D8E150>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000230C393DFA0>, root_client=<openai.OpenAI object at 0x0000023128CBD3A0>, root_async_client=<openai.AsyncOpenAI object at 0x0000023128D8F890>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, document loaders are tools used to load data from a source as `Document` objects. A `Document` consists of a piece of text and associated metadata. Document loaders can handle various types of data sources, including simple `.txt` files, web pages, or transcripts of YouTube videos. They expose two primary methods:\n",
      "\n",
      "1. \"Load\": This method loads documents from the configured source.\n",
      "2. \"Load and split\": This method loads documents from the configured source and splits them using a specified text splitter.\n",
      "\n",
      "Additionally, document loaders can optionally implement a \"lazy load\" method, which allows documents to be loaded into memory lazily, meaning only as needed rather than all at once.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"which method do we use to load documents using docment loaders in lang chain\"\n",
    "})\n",
    "\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'which method do we use to load documents using docment loaders in lang chain',\n",
       " 'context': [Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='Document loaders | ü¶úÔ∏èüîó LangChain'),\n",
       "  Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='implement a \"lazy load\" as well for lazily loading data into memory.Get started\\u200bThe simplest loader reads in a file as text and places it all into one document.from langchain_community.document_loaders import TextLoaderloader = TextLoader(\"./index.md\")loader.load()API Reference:TextLoader[    Document(page_content=\\'---\\\\nsidebar_position: 0\\\\n---\\\\n# Document loaders\\\\n\\\\nUse document loaders to load data from a source as `Document`\\\\\\'s. A `Document` is a piece of text\\\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\\\n\\\\nEvery document loader exposes two methods:\\\\n1. \"Load\": load documents from the configured source\\\\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\\\n\\\\nThey optionally implement:\\\\n\\\\n3. \"Lazy load\": load documents into memory lazily\\\\n\\', metadata={\\'source\\':'),\n",
       "  Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally'),\n",
       "  Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/', 'title': 'Document loaders | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.', 'language': 'en'}, page_content='load documents from the configured source and split them using the passed in text splitter\\\\n\\\\nThey optionally implement:\\\\n\\\\n3. \"Lazy load\": load documents into memory lazily\\\\n\\', metadata={\\'source\\': \\'../docs/docs/modules/data_connection/document_loaders/index.md\\'})]Help us out by providing feedback on this documentation page:PreviousRetrievalNextDocument loadersGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')],\n",
       " 'answer': 'Based on the provided context, document loaders are tools used to load data from a source as `Document` objects. A `Document` consists of a piece of text and associated metadata. Document loaders can handle various types of data sources, including simple `.txt` files, web pages, or transcripts of YouTube videos. They expose two primary methods:\\n\\n1. \"Load\": This method loads documents from the configured source.\\n2. \"Load and split\": This method loads documents from the configured source and splits them using a specified text splitter.\\n\\nAdditionally, document loaders can optionally implement a \"lazy load\" method, which allows documents to be loaded into memory lazily, meaning only as needed rather than all at once.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
